{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 패키지 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import pandas_datareader.data as web\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import torch.backends\n",
    "\n",
    "# 경고 메세지가 출력되지 않도록 만듦.\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "os.chdir(\"../../..\")\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import random\n",
    "import shutil\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 불러오기 및 웨이블릿 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:/Users/daily/Desktop/yfinance_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 134\u001b[0m\n\u001b[0;32m    131\u001b[0m data \u001b[38;5;241m=\u001b[39m data2\n\u001b[0;32m    132\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 134\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:/Users/daily/Desktop/yfinance_data.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\daily\\anaconda3\\envs\\venv_finrl\\lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\daily\\anaconda3\\envs\\venv_finrl\\lib\\site-packages\\pandas\\core\\generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3965\u001b[0m )\n\u001b[1;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\daily\\anaconda3\\envs\\venv_finrl\\lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mc:\\Users\\daily\\anaconda3\\envs\\venv_finrl\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    268\u001b[0m     )\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32mc:\\Users\\daily\\anaconda3\\envs\\venv_finrl\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:/Users/daily/Desktop/yfinance_data.csv'"
     ]
    }
   ],
   "source": [
    "acc = 'cpu'\n",
    "\n",
    "# 최대한의 시작 날짜 설정\n",
    "start_date = '2000-01-01'\n",
    "end_date = '2024-07-26'\n",
    "\n",
    "# S&P 500 ETF 데이터 불러오기\n",
    "snp500 = yf.download('SPY', start=start_date, end=end_date)['Close']\n",
    "\n",
    "# 미국 실업률 데이터 불러오기\n",
    "unemployment_rate = web.DataReader('UNRATE', 'fred', start_date, end_date)\n",
    "\n",
    "# 기준금리 데이터 불러오기\n",
    "interest_rate = web.DataReader('FEDFUNDS', 'fred', start_date, end_date)\n",
    "\n",
    "# 장단기 금리차 데이터 불러오기\n",
    "term_spread = web.DataReader('T10Y2Y', 'fred', start_date, end_date)\n",
    "\n",
    "# TIPS 데이터 불러오기\n",
    "tips = web.DataReader('DFII10', 'fred', start_date, end_date)\n",
    "\n",
    "# 하이일드 스프레드 데이터 불러오기\n",
    "high_yield_spread = web.DataReader('BAMLH0A0HYM2', 'fred', start_date, end_date)\n",
    "\n",
    "# WTI 원유 데이터 불러오기\n",
    "wti = web.DataReader('DCOILWTICO', 'fred', start_date, end_date)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 데이터를 거래일 기준으로 리샘플링 및 NA 값 처리\n",
    "snp500 = snp500.asfreq('B').fillna(method='ffill')\n",
    "unemployment_rate = unemployment_rate.asfreq('B').fillna(method='ffill')\n",
    "interest_rate = interest_rate.asfreq('B').fillna(method='ffill')\n",
    "term_spread = term_spread.asfreq('B').fillna(method='ffill')\n",
    "tips = tips.asfreq('B').fillna(method='ffill')\n",
    "high_yield_spread = high_yield_spread.asfreq('B').fillna(method='ffill')\n",
    "wti = wti.asfreq('B').fillna(method='ffill')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 원본 데이터프레임 결합 (data1)\n",
    "data1 = pd.concat([snp500, unemployment_rate, interest_rate, term_spread, tips, high_yield_spread, wti], axis=1)\n",
    "data1.columns = ['SNP500', 'Unemployment_Rate', 'Interest_Rate', 'Term_Spread', 'TIPS', 'High_Yield_Spread', 'WTI']\n",
    "data1 = data1.dropna()\n",
    "\n",
    "\n",
    "# 로그 변화율 계산 함수\n",
    "def calculate_log_return(series):\n",
    "    return np.log(series / series.shift(1))\n",
    "\n",
    "# 단순 변화율 계산 함수 (S&P 500, WTI에 사용)\n",
    "def calculate_percentage_change(series):\n",
    "    return series.pct_change() * 100\n",
    "\n",
    "# 단순 변화량 계산 함수 (나머지 변수에 사용)\n",
    "def calculate_difference(series):\n",
    "    return series.diff()\n",
    "\n",
    "# 변화율 및 변화량 계산\n",
    "data2 = pd.DataFrame(index=data1.index)\n",
    "data2['SNP500_log_return'] = calculate_log_return(data1['SNP500'])  # S&P 500 로그 변화율\n",
    "data2['TIPS_diff'] = calculate_difference(data1['TIPS'])  # TIPS 변화량\n",
    "data2['Unemployment_Rate_diff'] = calculate_difference(data1['Unemployment_Rate'])  # 실업률 변화량\n",
    "data2['Interest_Rate_diff'] = calculate_difference(data1['Interest_Rate'])  # 금리 변화량\n",
    "data2['Term_Spread_diff'] = calculate_difference(data1['Term_Spread'])  # 장단기 금리차 변화량\n",
    "data2['High_Yield_Spread_diff'] = calculate_difference(data1['High_Yield_Spread'])  # 하이일드 스프레드 변화량\n",
    "data2['WTI_diff'] = calculate_percentage_change(data1['WTI'])  # WTI 단순 변화율\n",
    "\n",
    "# NaN 값 제거\n",
    "data2 = data2.dropna()\n",
    "\n",
    "# 원본 데이터와 계산된 변화율, 변화량 합침\n",
    "data_combined = pd.concat([data1, data2], axis=1)\n",
    "\n",
    "# 결측값 제거\n",
    "data_combined = data_combined.dropna()\n",
    "\n",
    "# 주별 리샘플링\n",
    "data_weekly = data_combined.resample('W').last()\n",
    "\n",
    "# 주별 로그 변화율 및 단순 변화율, 변화량 다시 계산\n",
    "data_weekly['SNP500_log_return'] = calculate_log_return(data_weekly['SNP500'])  # S&P 500 로그 변화율\n",
    "data_weekly['TIPS_diff'] = calculate_difference(data_weekly['TIPS'])  # TIPS 변화량\n",
    "data_weekly['Unemployment_Rate_diff'] = calculate_difference(data_weekly['Unemployment_Rate'])  # 실업률 변화량\n",
    "data_weekly['Interest_Rate_diff'] = calculate_difference(data_weekly['Interest_Rate'])  # 금리 변화량\n",
    "data_weekly['Term_Spread_diff'] = calculate_difference(data_weekly['Term_Spread'])  # 장단기 금리차 변화량\n",
    "data_weekly['High_Yield_Spread_diff'] = calculate_difference(data_weekly['High_Yield_Spread'])  # 하이일드 스프레드 변화량\n",
    "data_weekly['WTI_diff'] = calculate_percentage_change(data_weekly['WTI'])  # WTI 단순 변화율\n",
    "\n",
    "# NaN 값 제거\n",
    "data_weekly = data_weekly.dropna()\n",
    "\n",
    "# 앞의 22개 데이터 잘라내기\n",
    "data_weekly = data_weekly.iloc[22:]\n",
    "\n",
    "# 노이즈 제거 및 최소-최대 정규화 적용\n",
    "wv = 'coif5'\n",
    "scaler = StandardScaler()\n",
    "data_final = pd.DataFrame(index=data_combined.index)\n",
    "\n",
    "for column in data_combined.columns:\n",
    "    # 노이즈 제거 과정\n",
    "    signal = data_combined[column].values\n",
    "    N = len(signal)\n",
    "    lev = pywt.dwt_max_level(N, wv)\n",
    "    coeffs = pywt.wavedec(signal, wv, level=lev)\n",
    "    D1 = coeffs[-1]\n",
    "    sigma_med = np.median(np.abs(D1)) / 0.6745\n",
    "    lambda_U = sigma_med * np.sqrt(2 * np.log(N))\n",
    "    coeffs = [pywt.threshold(c, lambda_U, mode='garrote') for c in coeffs]\n",
    "    denoised_signal = pywt.waverec(coeffs, wv)\n",
    "\n",
    "    # 원래 신호의 길이에 맞추기\n",
    "    denoised_signal = denoised_signal[:N]\n",
    "\n",
    "    # 최소-최대 정규화 적용 후 data_final에 저장\n",
    "    data_final[column] = scaler.fit_transform(denoised_signal.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "data2 = data_final\n",
    "\n",
    "# 인덱스를 열로 복사하여 Date 열 생성\n",
    "data2['Date'] = data2.index\n",
    "\n",
    "# Date 열의 데이터 타입을 datetime으로 변환\n",
    "data2['Date'] = pd.to_datetime(data2['Date'])\n",
    "\n",
    "# 데이터 생성\n",
    "data = data2\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "\n",
    "data.to_csv('C:/Users/daily/Desktop/yfinance_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training 시계열 데이터 생성성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplot 한글 출력 코드\n",
    "plt.rcParams['font.family'] ='Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] =False\n",
    "\n",
    "data = pd.read_csv('C:/Users/daily/Desktop/yfinance_data.csv')\n",
    "\n",
    "# 데이터 생성\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "\n",
    "data[\"time_idx\"] = data[\"Date\"].dt.year * 12 + data[\"Date\"].dt.month\n",
    "data[\"time_idx\"] -= data[\"time_idx\"].min()\n",
    "\n",
    "# year로 범주형 변수 생성\n",
    "data['month'] = data.Date.dt.month.astype(str).astype('category')\n",
    "\n",
    "max_prediction_length = 25 # 예측 일수\n",
    "max_encoder_length = 100 # 학습하는 과거 데이터 일수\n",
    "training_cutoff = data[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "# 인코더 : 시계열 데이터의 초기 부분을 입력 받아 특징 추출\n",
    "# 디코더 : 인코더가 생성한 벡터 입력 받아 원하는 출력 시퀀스 생성 (다음 단계 예측)\n",
    "\n",
    "# training 시계열 데이터 생성\n",
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= training_cutoff], # data 중 time_idx가 training_cutoff 이하인 데이터만 추출\n",
    "    allow_missing_timesteps=True, # 누락된 시계열 구간 허용\n",
    "    time_idx=\"time_idx\", # 시간 인덱스 열 설정\n",
    "    target=\"SNP500\", # 타겟 열 설정\n",
    "    group_ids= ['month'], # 그룹화할 데이터 설정(범주형 변수일 것)\n",
    "    min_encoder_length=max_encoder_length // 2,  # 인코더의 최소 길이 설정\n",
    "    max_encoder_length=max_encoder_length, # 인코더 최대 길이 설정\n",
    "    min_prediction_length=1, # 모델이 예측할 기간 -> 모델이 한 번의 예측에서 최소한으로 예측할 시간 간격\n",
    "    max_prediction_length=max_prediction_length,  # 모델이 한 번의 예측에서 최대로 예측할 수 있는 시간 간격\n",
    "    time_varying_known_categoricals=[], # 값을 알고 있는 범주형 동적변수\n",
    "    time_varying_known_reals=['Date', 'Unemployment_Rate', 'Interest_Rate', 'Term_Spread', 'TIPS', \n",
    "                                'High_Yield_Spread', 'WTI', 'SNP500_log_return',\n",
    "                                'TIPS_diff', 'Unemployment_Rate_diff', 'Interest_Rate_diff',\n",
    "                                'Term_Spread_diff', 'High_Yield_Spread_diff', 'WTI_diff'], # 값을 알고 있는 연속형 동적 변수 지정\n",
    "    time_varying_unknown_categoricals=[], # 미지 범주형 동적변수 지정\n",
    "    time_varying_unknown_reals=[\"SNP500\"], # 미지 연속형 동적변수 지정\n",
    "    target_normalizer=GroupNormalizer( # groups = []로 지정한 범주형 변수에 따라 그룹을 나눠 정규화 수행\n",
    "       groups=['month'], transformation=None   # 정규화 변환 함수 = softplus(입력이 음수일 경우 0으로 변환되며, 양수일 경우 로그의 역함수로 변환)\n",
    "    ),\n",
    "    add_relative_time_idx=True, # 대적인 시간 인덱스(relative time index)를 데이터셋에 추가\n",
    "    add_target_scales=True, # 타겟 변수의 스케일(범위 , 크기)을 추가\n",
    "    add_encoder_length=True, # 인코더의 길이(encoder length)를 데이터셋에 추가\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 유효성 검증 세트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3330)\n"
     ]
    }
   ],
   "source": [
    "# 유효성 검증 세트 생성\n",
    "# training 학습 데이터셋을 기반으로 새로운 데이터셋 생성\n",
    "# training = 학습 데이터셋\n",
    "# data = 기존 데이터, 검증 & 테스트를 위해 사용\n",
    "# perdict = True : 예측용 데이터셋으로 설정\n",
    "# stop_randomization = True : 데이터셋 생성 과정 중 임의화 중단, 학습 데이터 셋에는 임의화가 일어나지만 검증&테스트 단계에서는 순서 유지하여 성능 평가\n",
    "validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)\n",
    "\n",
    "\n",
    "# create dataloaders for model\n",
    "# 한 번에 모델에 입력되는 데이터 묶음 개수 (32 to 128개로 설정)\n",
    "batch_size = 128\n",
    "# training 데이터셋을 dataloader 형태로 변환\n",
    "# train = True : 학습용 데이터로 설정\n",
    "# num_workers = 데이터 로딩을 위해 사용할 병렬 작업 수 설정 -> 0 : 별도의 병렬 처리 없이 데이터 처리\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "# 검증 데이터셋을 dataloader 형태로 변환\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)\n",
    "\n",
    "\n",
    "# 모델이 예측을 수행할 때, 이전 시간 단계의 마지막 값으로 다음 값을 예측하고, 이를 통해 계산된 MAE를 평가\n",
    "# Baseline() = 시계열 데이터의 이동 평균, 평균값 등의 간단한 통계적 기법을 사용하여 예측을 수행\n",
    "# val_dataloader(검증 데이터셋) 데이터에 대한 예측 수행\n",
    "# return_y = True : 실제 타겟 값도 반환\n",
    "baseline_predictions = Baseline().predict(val_dataloader, return_y=True)\n",
    "# output(예측값)과 y(실제 타겟 값) 간의 평균 절대 오차를 계산\n",
    "base_MAE = MAE()(baseline_predictions.output, baseline_predictions.y)\n",
    "print(base_MAE)\n",
    "\n",
    "# configure network and trainer\n",
    "\n",
    "# pl.seed_everything() = 실험의 재현성을 위해 사용되는 함수, 매번 동일한 시드값을 사용하게 함(랜덤성 제어)\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# pl.Trainer() = 모델 학습, 검증, 테스트 등의 루프를 관리\n",
    "# accelerator = 학습을 수행할 디바이스 지정\n",
    "# gradient_clip_val = 그래디언트 클리핑(gradient clipping) 값 설정, 그레디언트 폭주(발산) 방지\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"cpu\",\n",
    "    # clipping gradients is a hyperparameter and important to prevent divergance of the gradient for recurrent neural networks\n",
    "    gradient_clip_val=0.1,\n",
    "    callbacks=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFT 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 17.6k\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "`model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `TemporalFusionTransformer`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 29\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtuner\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tuner\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Tuner = 다양한 모델 파라미터를 튜닝하고 최적의 학습률을 찾음\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# lr_find = 학습률의 범위를 찾음\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mTuner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_find\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 튜닝할 대상 모델\u001b[39;49;00m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 학습 데이터 로드\u001b[39;49;00m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 검증 데이터 로드\u001b[39;49;00m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 탐색할 최대 학습률\u001b[39;49;00m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 탐색할 최소 학습률\u001b[39;49;00m\n\u001b[0;32m     35\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# 학습률 출력\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuggested learning rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres\u001b[38;5;241m.\u001b[39msuggestion()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\daily\\anaconda3\\envs\\venv_finrl\\lib\\site-packages\\lightning\\pytorch\\tuner\\tuning.py:180\u001b[0m, in \u001b[0;36mTuner.lr_find\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, dataloaders, datamodule, method, min_lr, max_lr, num_training, mode, early_stop_threshold, update_attr, attr_name)\u001b[0m\n\u001b[0;32m    177\u001b[0m lr_finder_callback\u001b[38;5;241m.\u001b[39m_early_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;241m=\u001b[39m [lr_finder_callback] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer\u001b[38;5;241m.\u001b[39mcallbacks\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;241m=\u001b[39m [cb \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;28;01mif\u001b[39;00m cb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lr_finder_callback]\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lr_finder_callback\u001b[38;5;241m.\u001b[39moptimal_lr\n",
      "File \u001b[1;32mc:\\Users\\daily\\anaconda3\\envs\\venv_finrl\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:533\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\n\u001b[0;32m    500\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    501\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    505\u001b[0m     ckpt_path: Optional[_PATH] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    506\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    507\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Runs the full optimization routine.\u001b[39;00m\n\u001b[0;32m    508\u001b[0m \n\u001b[0;32m    509\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    531\u001b[0m \n\u001b[0;32m    532\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 533\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_unwrap_optimized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m    535\u001b[0m     _verify_strategy_supports_compile(model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy)\n",
      "File \u001b[1;32mc:\\Users\\daily\\anaconda3\\envs\\venv_finrl\\lib\\site-packages\\pytorch_lightning\\utilities\\compile.py:111\u001b[0m, in \u001b[0;36m_maybe_unwrap_optimized\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m    110\u001b[0m _check_mixed_imports(model)\n\u001b[1;32m--> 111\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(model)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    113\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: `model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `TemporalFusionTransformer`"
     ]
    }
   ],
   "source": [
    "# TFT 모델 생성\n",
    "\n",
    "# from_dataset() = 주어진 설정 값으로 TFT 모델 생성\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training, # training = TFT 모델을 학습할 시계열 데이터셋, 모델 초기화와 학습에 사용\n",
    "    learning_rate=0.03, # 학습률 설정, 모델의 매개변수 업데이트 속도 조절\n",
    "    hidden_size=8,  # TFT 모델의 은닉층 크기, 모델의 복잡성 & 표현력 결정 (학습률을 위한 중요한 변수)\n",
    "    attention_head_size=1, # 어텐션 헤드의 크기 설정 (큰 데이터셋에는 4로 설정)\n",
    "    # 어텐션 메커니즘 수행에 사용되는 매개변수\n",
    "    # 어텐션 메커니즘 : 주어진 입력에 대해 각 입력 위치에 중요도를 부여하여 새로운 표현을 생성하는 방법, 쿼리(query), 키(key), 값(value)의 쌍이 사용\n",
    "    # 어텐션 헤드는 이러한 쿼리와 키에 대한 연산을 병렬로 수행하는 부분\n",
    "    # attention_head_size = 1 : 각 쿼리(query)에 대해 하나의 키(key)와 연결하여 어텐션 가중치(attention weight)를 계산하는 방식을 사용\n",
    "    dropout=0.1,  # 드롭아웃 확률 설정 (0.1 ~ 0.3으로 설정)\n",
    "    # 드롭아웃 : 학습 중 신경망의 일부 뉴런을 무작위 선택, 제외\n",
    "    hidden_continuous_size=8,  # 연속 변수의 은닉층 크기 설정, 시계열 데이터의 연속적 부분 처리 (hidden_size와 같은 값으로 설정)\n",
    "    loss=QuantileLoss(), # 손실 함수 설정, QuantileLoss = 특정 분위수(quantile)에 대한 예측 정확도를 향상\n",
    "    optimizer=\"Ranger\", # 최적화 알고리즘 설정, Ranger = 최적화의 속도와 안정을 개선 위한 발전된 기법 사용한 최적화 알고리즘\n",
    "    # reduce learning rate if no improvement in validation loss after x epochs\n",
    "    # reduce_on_plateau_patience=1000,\n",
    ")\n",
    "# tft 신경망 모델의 파라미터 수를 출력\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
    "\n",
    "\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "\n",
    "# Tuner = 다양한 모델 파라미터를 튜닝하고 최적의 학습률을 찾음\n",
    "# lr_find = 학습률의 범위를 찾음\n",
    "res = Tuner(trainer=trainer).lr_find(\n",
    "    tft, # 튜닝할 대상 모델\n",
    "    train_dataloaders=train_dataloader, # 학습 데이터 로드\n",
    "    val_dataloaders=val_dataloader, # 검증 데이터 로드\n",
    "    max_lr=10.0, # 탐색할 최대 학습률\n",
    "    min_lr=1e-6, # 탐색할 최소 학습률\n",
    ")\n",
    "\n",
    "# 학습률 출력\n",
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "# fig = plt.figure()\n",
    "plt.show()\n",
    "res_lr = res.suggestion()\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "\n",
    "# 학습 과정 중 검증 손실(val_loss)를 모니터링, 성능 개선 안 되었을 시 학습 중지\n",
    "# min_delta = 개선된 것으로 간주할 최소 손실 변화\n",
    "# patience = 성능이 개선되지 않은 상태를 얼마나 참을 수 있는지를 나타내는 숫자\n",
    "# verbose = True로 설정 시 EarlyStopping이 각 조건 충족 시 메시지를 출력\n",
    "# mode = 모니터링 지표의 최소화(min) 또는 최대화(max)를 나타냄\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "# 학습률 변화 기록, 출력\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "# 학습 로그 기록, 시각화\n",
    "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "\n",
    "# pytorch_lightning 이용 모델 학습\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10, # 학습 횟수\n",
    "    accelerator=\"cpu\", # CPU 사용하여 학습\n",
    "    enable_model_summary=True, # 모델의 요약 정보 출력\n",
    "    gradient_clip_val=0.1, # 그레디언트 클리핑 값 설정\n",
    "    limit_train_batches=50,  # 학습 중 실제로 사용할 학습 batch의 비율, 전체 학습 데이터셋의 50%만 사용\n",
    "    # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "    callbacks=[lr_logger, early_stop_callback], # 학습 중 호출 할 함수 전달 (학습률 모니터링, 조기종료)\n",
    "    logger=logger, # 학습 로그 기록할 로거 설정\n",
    ")\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=res_lr,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=2,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,  # 로깅 주기 설정\n",
    "    optimizer=\"Ranger\",\n",
    "    reduce_on_plateau_patience=4, #  학습 손실이 개선되지 않을 때 학습률을 조정하는 패션스(patience)를 설정, 4번의 에포크동안 학습 손실 개선 안될 시 학습률 줄임\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
    "\n",
    "# trainer = 모델 학습의 주체(tft 모델 학습 시 필요한 기능 제공, 제어 담당), tft = 실제 학습할 모델 객체\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")\n",
    "\n",
    "# 검증 손실에 의거한 최적 모델 로드\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "\n",
    "# 검증세트의 MAE 계산\n",
    "predictions = best_tft.predict(val_dataloader, return_y=True, trainer_kwargs=dict(accelerator=\"cpu\"))\n",
    "pred_MAE = MAE()(predictions.output, predictions.y)\n",
    "print(pred_MAE)\n",
    "\n",
    "pred_value = []\n",
    "act_value = []\n",
    "\n",
    "raw_predictions = best_tft.predict(training)\n",
    "\n",
    "plt.title('actual vs predict (SNP500)')\n",
    "plt.plot(raw_predictions, label = 'prediction', color = 'blue', lw = 1)\n",
    "plt.plot(data[\"SNP500\"], label = 'actual', color = 'red', lw = 1)\n",
    "plt.xlabel('time')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_finrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
